{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yyamada/miniconda3/envs/SAA/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "# from dataset import get_data_transforms, get_strong_transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from PIL import Image\n",
    "from statistics import mean\n",
    "from skimage import measure\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import auc\n",
    "from torch.utils.data import DataLoader\n",
    "from models.resnet import resnet18, resnet34, resnet50, wide_resnet50_2, resnext50_32x4d\n",
    "from models.de_resnet import de_wide_resnet50_2, de_resnet18, de_resnet34, de_resnet50, de_resnext50_32x4d\n",
    "from models.recontrast import ReContrast, ReContrast\n",
    "from dataset import MVTecDataset\n",
    "import torch.backends.cudnn as cudnn\n",
    "import argparse\n",
    "# from utils import evaluation, visualize, global_cosine, global_cosine_hm\n",
    "from torch.nn import functional as F\n",
    "from functools import partial\n",
    "from ptflops import get_model_complexity_info\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "import copy\n",
    "import logging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1クラスごとにproを評価"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使い方\n",
    "##### 1. バッチサイズ1のtest_loaderと推論モデル用意\n",
    "##### 2. compute_pro関数の引数としてGT、推論マスクを渡す\n",
    "##### 3. GT.shap → torch.Size([1, 1, 256, 256]) 各値は正常領域False, 異常領域True (gt=gt.bool()使用)\n",
    "##### 4. mask.shape → (256,256) 各値は実数 (mask.shape → (1, 256, 256)も一部変更すれば可能)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mvtec\n",
    "\n",
    "def get_data_transforms(image_size, crop_size, mean_train=None, std_train=None):\n",
    "    mean_train = [0.485, 0.456, 0.406] if mean_train is None else mean_train\n",
    "    std_train = [0.229, 0.224, 0.225] if std_train is None else std_train\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.Normalize(mean=mean_train,\n",
    "                             std=std_train)])\n",
    "    gt_transforms = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor()])\n",
    "    return data_transforms, gt_transforms\n",
    "\n",
    "class MVTecDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform, gt_transform, phase):\n",
    "        if phase == 'train':\n",
    "            self.img_path = os.path.join(root, 'train')\n",
    "        else:\n",
    "            self.img_path = os.path.join(root, 'test')\n",
    "            self.gt_path = os.path.join(root, 'ground_truth')\n",
    "        self.transform = transform\n",
    "        self.gt_transform = gt_transform\n",
    "        # load dataset\n",
    "        self.img_paths, self.gt_paths, self.labels, self.types = self.load_dataset()  # self.labels => good : 0, anomaly : 1\n",
    "\n",
    "    def load_dataset(self):\n",
    "\n",
    "        img_tot_paths = []\n",
    "        gt_tot_paths = []\n",
    "        tot_labels = []\n",
    "        tot_types = []\n",
    "\n",
    "        defect_types = os.listdir(self.img_path)\n",
    "\n",
    "        for defect_type in defect_types:\n",
    "            if defect_type == 'good':\n",
    "                img_paths = glob.glob(os.path.join(self.img_path, defect_type) + \"/*.png\") + \\\n",
    "                            glob.glob(os.path.join(self.img_path, defect_type) + \"/*.JPG\")\n",
    "                img_tot_paths.extend(img_paths)\n",
    "                gt_tot_paths.extend([0] * len(img_paths))\n",
    "                tot_labels.extend([0] * len(img_paths))\n",
    "                tot_types.extend(['good'] * len(img_paths))\n",
    "            else:\n",
    "                img_paths = glob.glob(os.path.join(self.img_path, defect_type) + \"/*.png\") + \\\n",
    "                            glob.glob(os.path.join(self.img_path, defect_type) + \"/*.JPG\")\n",
    "                gt_paths = glob.glob(os.path.join(self.gt_path, defect_type) + \"/*.png\")\n",
    "                img_paths.sort()\n",
    "                gt_paths.sort()\n",
    "                img_tot_paths.extend(img_paths)\n",
    "                gt_tot_paths.extend(gt_paths)\n",
    "                tot_labels.extend([1] * len(img_paths))\n",
    "                tot_types.extend([defect_type] * len(img_paths))\n",
    "\n",
    "        assert len(img_tot_paths) == len(gt_tot_paths), \"Something wrong with test and ground truth pair!\"\n",
    "\n",
    "        return img_tot_paths, gt_tot_paths, tot_labels, tot_types\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, gt, label, img_type = self.img_paths[idx], self.gt_paths[idx], self.labels[idx], self.types[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        if gt == 0:\n",
    "            gt = torch.zeros([1, img.size()[-2], img.size()[-2]])\n",
    "        else:\n",
    "            gt = Image.open(gt)\n",
    "            gt = self.gt_transform(gt)\n",
    "\n",
    "        assert img.size()[1:] == gt.size()[1:], \"image.size != gt.size !!!\"\n",
    "\n",
    "        return img, gt, label, img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compute\n",
    "\n",
    "def cal_anomaly_map(fs_list, ft_list, out_size=224, amap_mode='mul', log=False):\n",
    "    if not isinstance(out_size, tuple):\n",
    "        out_size = (out_size, out_size)\n",
    "    if amap_mode == 'mul':\n",
    "        anomaly_map = np.ones(out_size)\n",
    "    else:\n",
    "        anomaly_map = np.zeros(out_size)\n",
    "\n",
    "    a_map_list = []\n",
    "    for i in range(len(ft_list)):\n",
    "        fs = fs_list[i]\n",
    "        ft = ft_list[i]\n",
    "        a_map = 1 - F.cosine_similarity(fs, ft)\n",
    "        a_map = torch.unsqueeze(a_map, dim=1)\n",
    "        a_map = F.interpolate(a_map, size=out_size, mode='bilinear', align_corners=True)\n",
    "        a_map = a_map[0, 0, :, :].to('cpu').detach().numpy()\n",
    "        a_map_list.append(a_map)\n",
    "        if amap_mode == 'mul':\n",
    "            anomaly_map *= a_map\n",
    "        else:\n",
    "            anomaly_map += a_map\n",
    "    return anomaly_map, a_map_list\n",
    "\n",
    "def compute_pro(masks: ndarray, amaps: ndarray, num_th: int = 200) -> None:\n",
    "    \"\"\"Compute the area under the curve of per-region overlaping (PRO) and 0 to 0.3 FPR\n",
    "    Args:\n",
    "        category (str): Category of product\n",
    "        masks (ndarray): All binary masks in test. masks.shape -> (num_test_data, h, w)\n",
    "        amaps (ndarray): All anomaly maps in test. amaps.shape -> (num_test_data, h, w)\n",
    "        num_th (int, optional): Number of thresholds\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(amaps, ndarray), \"type(amaps) must be ndarray\"\n",
    "    assert isinstance(masks, ndarray), \"type(masks) must be ndarray\"\n",
    "    assert amaps.ndim == 3, \"amaps.ndim must be 3 (num_test_data, h, w)\"\n",
    "    assert masks.ndim == 3, \"masks.ndim must be 3 (num_test_data, h, w)\"\n",
    "    assert amaps.shape == masks.shape, \"amaps.shape and masks.shape must be same\"\n",
    "    assert set(masks.flatten()) == {0, 1}, \"set(masks.flatten()) must be {0, 1}\"\n",
    "    assert isinstance(num_th, int), \"type(num_th) must be int\"\n",
    "\n",
    "    df = pd.DataFrame([], columns=[\"pro\", \"fpr\", \"threshold\"])\n",
    "    binary_amaps = np.zeros_like(amaps, dtype=bool)\n",
    "\n",
    "    min_th = amaps.min()\n",
    "    max_th = amaps.max()\n",
    "    delta = (max_th - min_th) / num_th\n",
    "\n",
    "    for th in np.arange(min_th, max_th, delta):\n",
    "        # 2値化処理(segmentation)\n",
    "        binary_amaps[amaps <= th] = 0\n",
    "        binary_amaps[amaps > th] = 1\n",
    "\n",
    "        # pro計算\n",
    "        pros = []\n",
    "        for binary_amap, mask in zip(binary_amaps, masks):\n",
    "            for region in measure.regionprops(measure.label(mask)):\n",
    "                axes0_ids = region.coords[:, 0]\n",
    "                axes1_ids = region.coords[:, 1]\n",
    "                tp_pixels = binary_amap[axes0_ids, axes1_ids].sum()\n",
    "                pros.append(tp_pixels / region.area)\n",
    "\n",
    "        # fpr計算\n",
    "        inverse_masks = 1 - masks\n",
    "        fp_pixels = np.logical_and(inverse_masks, binary_amaps).sum()\n",
    "        fpr = fp_pixels / inverse_masks.sum()\n",
    "\n",
    "        df = df.append({\"pro\": mean(pros), \"fpr\": fpr, \"threshold\": th}, ignore_index=True)\n",
    "\n",
    "    # Normalize FPR from 0 ~ 1 to 0 ~ 0.3\n",
    "    df = df[df[\"fpr\"] < 0.3]\n",
    "    df[\"fpr\"] = df[\"fpr\"] / df[\"fpr\"].max()\n",
    "\n",
    "    pro_auc = auc(df[\"fpr\"], df[\"pro\"])\n",
    "    return pro_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "[['toothbrush', 0.9868, 0.9806, 0.9115]]\n",
      "mPixel Auroc:0.9868, mSample Auroc:0.9806, mPixel Aupro:0.9115\n"
     ]
    }
   ],
   "source": [
    "# テストデータ読み込み\n",
    "_class_ ='toothbrush'\n",
    "test_path = '/home/data/mvtec/' + _class_\n",
    "data_transform, gt_transform = get_data_transforms(image_size=256, crop_size=256)\n",
    "test_data = MVTecDataset(root=test_path, transform=data_transform, gt_transform=gt_transform, phase=\"test\")\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "# 推論モデル用意\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "weight_path = 'weights/model.pth'\n",
    "encoder, bn = wide_resnet50_2(pretrained=True)\n",
    "decoder = de_wide_resnet50_2(pretrained=False, output_conv=2)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "bn = bn.to(device)\n",
    "decoder = decoder.to(device)\n",
    "encoder_freeze = copy.deepcopy(encoder)\n",
    "\n",
    "model = ReContrast(encoder=encoder, encoder_freeze=encoder_freeze, bottleneck=bn, decoder=decoder)\n",
    "model.load_state_dict(torch.load(weight_path))\n",
    "\n",
    "# 推論\n",
    "model.eval()\n",
    "gt_list_px = []\n",
    "pr_list_px = []\n",
    "gt_list_sp = []\n",
    "pr_list_sp = []\n",
    "aupro_list = []\n",
    "result_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img, gt, label, _ in test_dataloader:\n",
    "        img = img.to(device)\n",
    "\n",
    "        en, de = model(img)\n",
    "\n",
    "        anomaly_map, _ = cal_anomaly_map(en, de, img.shape[-1], amap_mode='a')\n",
    "        anomaly_map = gaussian_filter(anomaly_map, sigma=4)\n",
    "        # gt[gt > 0.5] = 1\n",
    "        # gt[gt <= 0.5] = 0\n",
    "        gt = gt.bool()\n",
    "\n",
    "        # pro計算\n",
    "        if label.item() != 0:\n",
    "            # print(label.item()) # 1\n",
    "            # print(gt.shape) # torch.Size([1, 1, 256, 256])\n",
    "            # print(anomaly_map.shape) # (256, 256)\n",
    "            # print(gt)\n",
    "            # print(anomaly_map)\n",
    "            \n",
    "            aupro_list.append(compute_pro(gt.squeeze(0).cpu().numpy().astype(int),\n",
    "                                            anomaly_map[np.newaxis, :, :]))\n",
    "        gt_list_px.extend(gt.cpu().numpy().astype(int).ravel())\n",
    "        pr_list_px.extend(anomaly_map.ravel())\n",
    "        gt_list_sp.append(np.max(gt.cpu().numpy().astype(int)))\n",
    "        pr_list_sp.append(np.max(anomaly_map))\n",
    "\n",
    "auroc_px = round(roc_auc_score(gt_list_px, pr_list_px), 4)\n",
    "auroc_sp = round(roc_auc_score(gt_list_sp, pr_list_sp), 4)\n",
    "aupro_px = round(np.mean(aupro_list), 4)\n",
    "\n",
    "result_list.append([_class_, auroc_px, auroc_sp, aupro_px])\n",
    "    \n",
    "mean_auroc_px = np.mean([result[1] for result in result_list])\n",
    "mean_auroc_sp = np.mean([result[2] for result in result_list])\n",
    "mean_aupro_px = np.mean([result[3] for result in result_list])\n",
    "print(result_list)\n",
    "print('mPixel Auroc:{:.4f}, mSample Auroc:{:.4f}, mPixel Aupro:{:.4}'.format(mean_auroc_px, mean_auroc_sp,\n",
    "                                                                                    mean_aupro_px))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全クラス一気にproを評価<br>(evaluation関数にmodelとtest_loaderごと渡す)\n",
    "##### ※ modelを変更する場合、evaluation関数のmodel推論部分の修正必要あり"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mvtec\n",
    "\n",
    "def get_data_transforms(image_size, crop_size, mean_train=None, std_train=None):\n",
    "    mean_train = [0.485, 0.456, 0.406] if mean_train is None else mean_train\n",
    "    std_train = [0.229, 0.224, 0.225] if std_train is None else std_train\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.Normalize(mean=mean_train,\n",
    "                             std=std_train)])\n",
    "    gt_transforms = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor()])\n",
    "    return data_transforms, gt_transforms\n",
    "\n",
    "class MVTecDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform, gt_transform, phase):\n",
    "        if phase == 'train':\n",
    "            self.img_path = os.path.join(root, 'train')\n",
    "        else:\n",
    "            self.img_path = os.path.join(root, 'test')\n",
    "            self.gt_path = os.path.join(root, 'ground_truth')\n",
    "        self.transform = transform\n",
    "        self.gt_transform = gt_transform\n",
    "        # load dataset\n",
    "        self.img_paths, self.gt_paths, self.labels, self.types = self.load_dataset()  # self.labels => good : 0, anomaly : 1\n",
    "\n",
    "    def load_dataset(self):\n",
    "\n",
    "        img_tot_paths = []\n",
    "        gt_tot_paths = []\n",
    "        tot_labels = []\n",
    "        tot_types = []\n",
    "\n",
    "        defect_types = os.listdir(self.img_path)\n",
    "\n",
    "        for defect_type in defect_types:\n",
    "            if defect_type == 'good':\n",
    "                img_paths = glob.glob(os.path.join(self.img_path, defect_type) + \"/*.png\") + \\\n",
    "                            glob.glob(os.path.join(self.img_path, defect_type) + \"/*.JPG\")\n",
    "                img_tot_paths.extend(img_paths)\n",
    "                gt_tot_paths.extend([0] * len(img_paths))\n",
    "                tot_labels.extend([0] * len(img_paths))\n",
    "                tot_types.extend(['good'] * len(img_paths))\n",
    "            else:\n",
    "                img_paths = glob.glob(os.path.join(self.img_path, defect_type) + \"/*.png\") + \\\n",
    "                            glob.glob(os.path.join(self.img_path, defect_type) + \"/*.JPG\")\n",
    "                gt_paths = glob.glob(os.path.join(self.gt_path, defect_type) + \"/*.png\")\n",
    "                img_paths.sort()\n",
    "                gt_paths.sort()\n",
    "                img_tot_paths.extend(img_paths)\n",
    "                gt_tot_paths.extend(gt_paths)\n",
    "                tot_labels.extend([1] * len(img_paths))\n",
    "                tot_types.extend([defect_type] * len(img_paths))\n",
    "\n",
    "        assert len(img_tot_paths) == len(gt_tot_paths), \"Something wrong with test and ground truth pair!\"\n",
    "\n",
    "        return img_tot_paths, gt_tot_paths, tot_labels, tot_types\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, gt, label, img_type = self.img_paths[idx], self.gt_paths[idx], self.labels[idx], self.types[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        if gt == 0:\n",
    "            gt = torch.zeros([1, img.size()[-2], img.size()[-2]])\n",
    "        else:\n",
    "            gt = Image.open(gt)\n",
    "            gt = self.gt_transform(gt)\n",
    "\n",
    "        assert img.size()[1:] == gt.size()[1:], \"image.size != gt.size !!!\"\n",
    "\n",
    "        return img, gt, label, img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_anomaly_map(fs_list, ft_list, out_size=224, amap_mode='mul', log=False):\n",
    "    if not isinstance(out_size, tuple):\n",
    "        out_size = (out_size, out_size)\n",
    "    if amap_mode == 'mul':\n",
    "        anomaly_map = np.ones(out_size)\n",
    "    else:\n",
    "        anomaly_map = np.zeros(out_size)\n",
    "\n",
    "    a_map_list = []\n",
    "    for i in range(len(ft_list)):\n",
    "        fs = fs_list[i]\n",
    "        ft = ft_list[i]\n",
    "        a_map = 1 - F.cosine_similarity(fs, ft)\n",
    "        a_map = torch.unsqueeze(a_map, dim=1)\n",
    "        a_map = F.interpolate(a_map, size=out_size, mode='bilinear', align_corners=True)\n",
    "        a_map = a_map[0, 0, :, :].to('cpu').detach().numpy()\n",
    "        a_map_list.append(a_map)\n",
    "        if amap_mode == 'mul':\n",
    "            anomaly_map *= a_map\n",
    "        else:\n",
    "            anomaly_map += a_map\n",
    "    return anomaly_map, a_map_list\n",
    "\n",
    "def compute_pro(masks: ndarray, amaps: ndarray, num_th: int = 200) -> None:\n",
    "    \"\"\"Compute the area under the curve of per-region overlaping (PRO) and 0 to 0.3 FPR\n",
    "    Args:\n",
    "        category (str): Category of product\n",
    "        masks (ndarray): All binary masks in test. masks.shape -> (num_test_data, h, w)\n",
    "        amaps (ndarray): All anomaly maps in test. amaps.shape -> (num_test_data, h, w)\n",
    "        num_th (int, optional): Number of thresholds\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(amaps, ndarray), \"type(amaps) must be ndarray\"\n",
    "    assert isinstance(masks, ndarray), \"type(masks) must be ndarray\"\n",
    "    assert amaps.ndim == 3, \"amaps.ndim must be 3 (num_test_data, h, w)\"\n",
    "    assert masks.ndim == 3, \"masks.ndim must be 3 (num_test_data, h, w)\"\n",
    "    assert amaps.shape == masks.shape, \"amaps.shape and masks.shape must be same\"\n",
    "    assert set(masks.flatten()) == {0, 1}, \"set(masks.flatten()) must be {0, 1}\"\n",
    "    assert isinstance(num_th, int), \"type(num_th) must be int\"\n",
    "\n",
    "    df = pd.DataFrame([], columns=[\"pro\", \"fpr\", \"threshold\"])\n",
    "    binary_amaps = np.zeros_like(amaps, dtype=bool)\n",
    "\n",
    "    min_th = amaps.min()\n",
    "    max_th = amaps.max()\n",
    "    delta = (max_th - min_th) / num_th\n",
    "\n",
    "    for th in np.arange(min_th, max_th, delta):\n",
    "        binary_amaps[amaps <= th] = 0\n",
    "        binary_amaps[amaps > th] = 1\n",
    "\n",
    "        pros = []\n",
    "        for binary_amap, mask in zip(binary_amaps, masks):\n",
    "            for region in measure.regionprops(measure.label(mask)):\n",
    "                axes0_ids = region.coords[:, 0]\n",
    "                axes1_ids = region.coords[:, 1]\n",
    "                tp_pixels = binary_amap[axes0_ids, axes1_ids].sum()\n",
    "                pros.append(tp_pixels / region.area)\n",
    "\n",
    "        inverse_masks = 1 - masks\n",
    "        fp_pixels = np.logical_and(inverse_masks, binary_amaps).sum()\n",
    "        fpr = fp_pixels / inverse_masks.sum()\n",
    "\n",
    "        df = df.append({\"pro\": mean(pros), \"fpr\": fpr, \"threshold\": th}, ignore_index=True)\n",
    "\n",
    "    # Normalize FPR from 0 ~ 1 to 0 ~ 0.3\n",
    "    df = df[df[\"fpr\"] < 0.3]\n",
    "    df[\"fpr\"] = df[\"fpr\"] / df[\"fpr\"].max()\n",
    "\n",
    "    pro_auc = auc(df[\"fpr\"], df[\"pro\"])\n",
    "    return pro_auc\n",
    "\n",
    "def evaluation(model, dataloader, device, _class_=None, calc_pro=True):\n",
    "    model.eval()\n",
    "    gt_list_px = []\n",
    "    pr_list_px = []\n",
    "    gt_list_sp = []\n",
    "    pr_list_sp = []\n",
    "    aupro_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, gt, label, _ in dataloader:\n",
    "            img = img.to(device)\n",
    "\n",
    "            en, de = model(img)\n",
    "\n",
    "            anomaly_map, _ = cal_anomaly_map(en, de, img.shape[-1], amap_mode='a')\n",
    "            anomaly_map = gaussian_filter(anomaly_map, sigma=4)\n",
    "            # gt[gt > 0.5] = 1\n",
    "            # gt[gt <= 0.5] = 0\n",
    "            gt = gt.bool()\n",
    "\n",
    "            if calc_pro:\n",
    "                if label.item() != 0:\n",
    "                    # print(label.item()) # 1\n",
    "                    # print(gt.shape) # torch.Size([1, 1, 256, 256])\n",
    "                    # print(anomaly_map.shape) # (256, 256)\n",
    "                    # print(gt)\n",
    "                    # print(anomaly_map)\n",
    "                    aupro_list.append(compute_pro(gt.squeeze(0).cpu().numpy().astype(int),\n",
    "                                                  anomaly_map[np.newaxis, :, :]))\n",
    "            gt_list_px.extend(gt.cpu().numpy().astype(int).ravel())\n",
    "            pr_list_px.extend(anomaly_map.ravel())\n",
    "            gt_list_sp.append(np.max(gt.cpu().numpy().astype(int)))\n",
    "            pr_list_sp.append(np.max(anomaly_map))\n",
    "\n",
    "        auroc_px = round(roc_auc_score(gt_list_px, pr_list_px), 4)\n",
    "        auroc_sp = round(roc_auc_score(gt_list_sp, pr_list_sp), 4)\n",
    "\n",
    "    return auroc_px, auroc_sp, round(np.mean(aupro_list), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "toothbrush\n",
      "hazelnut\n",
      "[['toothbrush', 0.9868, 0.9806, 0.9115], ['hazelnut', 0.927, 0.9657, 0.8868]]\n",
      "mPixel Auroc:0.9569, mSample Auroc:0.9731, mPixel Aupro:0.8992\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# class_list = ['carpet', 'bottle', 'hazelnut', 'leather', 'cable', 'capsule', 'grid', 'pill',\n",
    "#                  'transistor', 'metal_nut', 'screw', 'toothbrush', 'zipper', 'tile', 'wood']\n",
    "class_list = ['toothbrush', 'hazelnut']\n",
    "weight_path = 'weights/model.pth'\n",
    "\n",
    "result_list = []\n",
    "for i, _class_ in enumerate(class_list):\n",
    "    print(_class_)\n",
    "    # テストデータ読み込み\n",
    "    test_path = '/home/data/mvtec/' + _class_\n",
    "    data_transform, gt_transform = get_data_transforms(image_size=256, crop_size=256)\n",
    "    test_data = MVTecDataset(root=test_path, transform=data_transform, gt_transform=gt_transform, phase=\"test\")\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False, num_workers=1)\n",
    "    \n",
    "    # 推論モデル用意\n",
    "    encoder, bn = wide_resnet50_2(pretrained=True)\n",
    "    decoder = de_wide_resnet50_2(pretrained=False, output_conv=2)\n",
    "\n",
    "    encoder = encoder.to(device)\n",
    "    bn = bn.to(device)\n",
    "    decoder = decoder.to(device)\n",
    "    encoder_freeze = copy.deepcopy(encoder)\n",
    "\n",
    "    model = ReContrast(encoder=encoder, encoder_freeze=encoder_freeze, bottleneck=bn, decoder=decoder)\n",
    "    model.load_state_dict(torch.load(weight_path)) # toothbrushで学習済み\n",
    "    \n",
    "    # 評価\n",
    "    auroc_px, auroc_sp, aupro_px = evaluation(model, test_dataloader, device)\n",
    "    result_list.append([_class_, auroc_px, auroc_sp, aupro_px])\n",
    "    \n",
    "mean_auroc_px = np.mean([result[1] for result in result_list])\n",
    "mean_auroc_sp = np.mean([result[2] for result in result_list])\n",
    "mean_aupro_px = np.mean([result[3] for result in result_list])\n",
    "print(result_list)\n",
    "print('mPixel Auroc:{:.4f}, mSample Auroc:{:.4f}, mPixel Aupro:{:.4}'.format(mean_auroc_px, mean_auroc_sp,\n",
    "                                                                                    mean_aupro_px))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
